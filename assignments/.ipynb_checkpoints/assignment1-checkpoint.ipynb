{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `caobd_s19` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Monday*, September 23, 2019, 23:59<br>\n",
    "**Peergrading deadline**: *Friday*, September 27, 2019, 23:59<br>\n",
    "**Peergrading feedback deadline**: *Sunday*, September 29, 2019, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex 1.1.10**: `dict`s and `defaultdict`s.\n",
    "1. What is a `defaultdict`? How would you say it is different from a normal Python `dict`?\n",
    "\n",
    ">A defaultdict works exactly like a normal dict, but it is initialized with a function (“default factory”) \n",
    "that takes no arguments and provides the default value for a nonexistent key. A defaultdict will never raise \n",
    "a KeyError. Any key that does not exist gets the value returned by the default factory.\n",
    "\n",
    "\n",
    "\n",
    "2. Write some code that takes a list of tuples:\n",
    "\n",
    ">        l = [(\"a\", 1), (\"b\", 3), (\"a\", None), (\"c\", False), (\"b\", True), (\"a\", None)]\n",
    "\n",
    ">     And produces a `defaultdict` object\n",
    "\n",
    ">        defaultdict(<type 'list'>, {'a': [1, None, None], 'c': [False], 'b': [3, True]})\n",
    "\n",
    ">*Hint: you can import `defaultdict` from `collections`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "l = [(\"a\", 1), (\"b\", 3), (\"a\", None), (\"c\", False), (\"b\", True), (\"a\", None)]\n",
    "\n",
    "d = defaultdict(list)\n",
    "\n",
    "for k, v in l:\n",
    "    d[k].append(v)\n",
    "        \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 1.2.4**: The URL reveals that the data is from reddit/r/gameofthrones, but can you recover that information from the data? Give your answer by 'keying' into the JSON data using square brackets.\n",
    "\n",
    ">*Hint: 'Keying' is a word i just made up. By it, I mean the following. Consider a JSON object such as:*\n",
    ">\n",
    ">        my_json_obj = {\n",
    ">            'cats': {\n",
    ">                'awesome': ['Missy'],\n",
    ">                'useless': ['Kim', 'Frank', 'Sandy']\n",
    ">            },\n",
    ">            'dogs': {\n",
    ">                'awesome': ['Finn', 'Dolores', 'Fido', 'Casper'],\n",
    ">                'useless': []\n",
    ">            }\n",
    ">        }\n",
    ">\n",
    ">*I can get the list of useless cats by keying into `my_json_obj` like such:*\n",
    ">\n",
    ">        >>> my_json_obj['cats']['useless']\n",
    ">        Out [ ]: ['Kim', 'Frank', 'Sandy']\n",
    ">\n",
    ">*`my_json_obj['cats']` returns the dictionary `{'awesome': ['Missy'], 'useless': ['Kim', 'Frank', 'Sandy']}` and getting '`useless`' from that eventually gives us `['Kim', 'Frank', 'Sandy']`. If any of those list items were a list of a dictionary themselves, we could have kept keying deeper into the structure.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "#grab data\n",
    "r = requests.get('https://www.reddit.com/r/gameofthrones/.json')\n",
    "print(r.status_code) #check if request worked\n",
    "\n",
    "#get json format\n",
    "data = r.json()\n",
    "\n",
    "#key into data\n",
    "print(data['data']['children'][0]['data']['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex 1.2.5**: Write two `for` loops (or list comprehensions for extra street credits) which:\n",
    ">1. Counts the number of spoilers.\n",
    ">2. Only prints headlines that aren't spoilers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = data['data']['children']\n",
    "\n",
    "count = 0\n",
    "\n",
    "for post in posts:\n",
    "    if post['data']['spoiler'] == True:\n",
    "        count+=1\n",
    "       \n",
    "print(\"[spoiler posts]:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.2**: The `get_x_y` function below gives you the number of comments versus score for the latest `N` posts on a given `subreddit`.\n",
    "3. In two seperate figures, floating side by side, scatter plot (left) the set of x and y variables for \"blackmirror\" and (right) x and y for \"news\". Remember to transform the data. My figure looks like [this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.2b.png).\n",
    "4. Comment on any differences you see in the trends. Why might number of comments versus post upvotes look different for a TV-show than for world news?\n",
    ">\n",
    ">*Hint: By \"transformation\" I explicitly mean that you map some function onto every value in a list of values. Example: I can apply a square root transformation like `x = [np.sqrt(v) for v in x]`. A faster way to do that, of course, would be just `x = np.sqrt(x)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T21:28:54.941490Z",
     "start_time": "2019-01-23T21:28:44.166852Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "\n",
    "def get_x_y(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            x.append(d['data']['num_comments'])\n",
    "            y.append(d['data']['score'])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return x, y\n",
    "                          \n",
    "x, y = get_x_y(\"news\", 500, count=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.4**: In Data Science, we often think of matrices as (usually two-dimensional) containers for data. Let's say we collect $N=500$ data points, that each has $M=10$ features. We can loslessly represent that data using an $N \\times M$ matrix, that is a matrix that has a row for each datapoint and a column for each feature. In fact, let's just go ahead and do that by altering the code of the `get_x_y` function from before a little bit.\n",
    ">\n",
    ">*Note: `numpy` has an object type called `matrix` but we rarely use that. Instead, we represent matrices as a `numpy` object type called `array`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T07:49:55.947679Z",
     "start_time": "2019-01-18T07:49:46.008308Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_matrix(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    X = []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            X.append([\n",
    "                int(d['data']['num_comments']),\n",
    "                int(d['data']['score']),\n",
    "                int(d['data']['ups']),\n",
    "                int(d['data']['downs']),\n",
    "                len(d['data']['selftext']),\n",
    "                len(d['data']['title']),\n",
    "                int(d['data']['is_original_content']),\n",
    "                int(d['data']['spoiler']),\n",
    "                int(d['data']['num_crossposts']),\n",
    "                int(d['data']['is_video'])\n",
    "            ])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return np.array(X)\n",
    "                          \n",
    "X = get_data_matrix(\"news\", 500, count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T07:49:57.572095Z",
     "start_time": "2019-01-18T07:49:57.567762Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here are the data, but how can we summarize them? Let's start by finding the so-called *covariance matrix*.\n",
    "1. Use the `np.cov` method on `X` to get its $10 \\times 10$ covariance matrix.\n",
    "2. Do you notice any characteristics of this matrix that are interesting? Comment.\n",
    "3. Plot the distribution of covariances, e.g. using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.5**: There's another use of the covariance matrix, other than just learning how features co-vary. In fact, it turns out that the *eigenvectors* of the covariance matrix are a set of mutually orthogonal vectors, that point in the directions of greatest variance in the data. The eigenvector with the greatest *eigenvalue* points along the direction of greatest variation, and so on. This is pretty neat, because if we know along which axes the data is most stretched, we can figure out how best to project it when visualizing it in 2D as a scatter plot! This whole procedure has a name: **Principal Component Analysis** (PCA) and it was invented by Karl Pearson in 1901. It belongs to a powerful class of linear algebra methods called **Matrix Factorization** methods. Ok, so rather than spending too much time on the math of PCA, let's just use the `sklearn` implementation and fit a PCA on `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T08:30:52.583430Z",
     "start_time": "2019-01-18T08:30:52.577358Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. Explain what the matrix you get when you call `pca.components_` means.\n",
    "2. Explain what the vector you get when you call `pca.explained_variance_ratio_` means. What insights about our data can we extract from this?\n",
    "3. Transform X using the PCA you just fitted, and scatter plot the first two dimensions of the transformed data. Please comment on what you see.\n",
    "3. Scatter plot dimensions 1 and 3 against each other. See something interesting now? What would be a way to figure out what the clusters represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.4**: People often use the p-value to gauge the *significance* of a given result. If the p-value of a result is low, the result is significant (which is good) and vice versa. Intuitively, the p-value measures the probabilty that a result *could have been obtained at random*, so you can imagine that if you find that the p-value for some result is HIGH (close to one), regardless of how cool it is, people will not care because, well, you just got lucky with that measurement, didn't you? I created two lists for you below, and you are going to find out if they are *significantly* correlated. You will be using the significance threshold 0.05 (which is arbitrary, disputed, yet very standard in the literature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T09:50:21.670665Z",
     "start_time": "2019-01-24T09:50:21.666647Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can incresae this to make the data more noisy (but let it be 4 for now)\n",
    "noise_level = 4\n",
    "\n",
    "# I'm just seeding the random number generator here, so we can compare results\n",
    "np.random.seed(0)\n",
    "\n",
    "# This is your data\n",
    "x = np.arange(0, 20) + np.random.normal(size=20) * noise_level\n",
    "y = np.arange(0, 20) + np.random.normal(size=20) * noise_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T09:55:18.924607Z",
     "start_time": "2019-01-24T09:55:18.922022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Why not make a scatter plot here, to see what you're working with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">4. Plot the p-value as a function of `noise_level` (let `noise_level` vary between 1 and 50), so we can see how our result becomes less and less significant as we increase the noise. Two questions: (1) at which value of `noise_level` does the correlation become insignificant, and (2) which p-value does the curve saturate at for large `noise_level`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 3.1.1**: From the Wikipedia API, get a list of all Marvel superheroes and another list of all Marvel supervillains. Use 'Category:Marvel_Comics_supervillains' and 'Category:Marvel_Comics_superheroes' to get the characters in each category.\n",
    "1. How many superheroes are there? How many supervillains?\n",
    "2. How many characters are both heroes and villains? What is the Jaccard similarity between the two groups?\n",
    "\n",
    ">*Hint: Google something like \"get list all pages in category wikimedia api\" if you're struggling with the query.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:\n",
    "\n",
    "> In the following cell, we define functions `retrieve_supervillains()` and `retrieve_superheroes()` which return lists of Marvel supervillains and superheroes, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "\n",
    "def retrieve_supervillains():\n",
    "    \"\"\"returns a dict of Marvel supervillains keyed by pageid\"\"\"\n",
    "    \n",
    "    sv_dict = {} # dict to hold supervillain data\n",
    "    gcm_continue = '' # continue key\n",
    "    \n",
    "    # base query\n",
    "    query = 'https://en.wikipedia.org/w/api.php?action=query&format=json&' \\\n",
    "            'generator=categorymembers&gcmtitle=Category:Marvel_Comics_supervillains&gcmlimit=max' \n",
    "    \n",
    "    while True:\n",
    "        # full query each iteration including the continue key when applicable\n",
    "        full_query = query + '&gcmcontinue=' + gcm_continue if gcm_continue!='' else query\n",
    "        \n",
    "        # update dict\n",
    "        villains_data = rq.get(full_query).json()\n",
    "        sv_dict.update(villains_data['query']['pages'])\n",
    "\n",
    "        # stop when no more results to load\n",
    "        if 'continue' not in villains_data:\n",
    "            break\n",
    "        else: # update continue key\n",
    "            gcm_continue = villains_data['continue']['gcmcontinue']\n",
    "            \n",
    "    return sv_dict\n",
    "\n",
    "def retrieve_superheroes():\n",
    "    \"\"\"returns a dict of Marvel superheroes keyed by pageid\"\"\"\n",
    "    \n",
    "    sh_dict = {} # dict to hold superhero data\n",
    "    gcm_continue = '' # continue key\n",
    "    \n",
    "    # base query\n",
    "    query = 'https://en.wikipedia.org/w/api.php?action=query&format=json&' \\\n",
    "            'generator=categorymembers&gcmtitle=Category:Marvel_Comics_superheroes&gcmlimit=max'\n",
    "\n",
    "    while True:\n",
    "        # full query each iteration including the continue key when applicable\n",
    "        full_query = query + '&gcmcontinue=' + gcm_continue if gcm_continue!='' else query   \n",
    "\n",
    "        # update dict\n",
    "        heroes_data = rq.get(full_query).json()\n",
    "        sh_dict.update(heroes_data['query']['pages'])\n",
    "\n",
    "        # stop when no more results to load\n",
    "        if 'continue' not in heroes_data:\n",
    "            break\n",
    "        else: # update continue key\n",
    "            gcm_continue = heroes_data['continue']['gcmcontinue']\n",
    "            \n",
    "    return sh_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we define the function `find_jaccard_sim(a, b)` which takes two sets as arguments and returns the jaccard similarity coefficient between the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jaccard_sim(a, b):\n",
    "    \"\"\"computes the jaccard similarity coefficient (aka jaccard index) of the two provided sets\n",
    "    \n",
    "    Args:\n",
    "        a: first set\n",
    "        b: second set\n",
    "    \n",
    "    Returns: \n",
    "        double: the jaccard similarity coefficient of sets a and b\n",
    "    \"\"\"\n",
    "    \n",
    "    intersect = a & b\n",
    "    union = a | b\n",
    "    \n",
    "    return len(intersect) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervillains = retrieve_supervillains()\n",
    "superheroes = retrieve_superheroes()\n",
    "\n",
    "# set objects of each dict's keys\n",
    "villains_set = set(supervillains.keys())\n",
    "heroes_set = set(superheroes.keys())\n",
    "ambiguous_chars_set = villains_set & heroes_set\n",
    "\n",
    "print('There are %d supervillains' % len(supervillains))\n",
    "print('There are %d superheroes' % len(superheroes))\n",
    "print('There are %d characters that are both supervillains and superheroes' % len(ambiguous_chars_set))\n",
    "\n",
    "\n",
    "jaccard_index = find_jaccard_sim(villains_set, heroes_set)\n",
    "\n",
    "print('The jaccard similarity coefficient of these two sets is %f' % jaccard_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are 1097 supervillains, 851 superheroes, and 152 characters that are both supervillains and superheroes. The jaccard similarity coefficient of these two sets is 0.084633."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.1**: Extract the length of the page of each character, and plot the distribution of this variable for each class (heroes/villains/ambiguous). Can you say anything about the popularity of characters in the Marvel universe based on your visualization?\n",
    "\n",
    ">*Hint: The simplest thing is to make a probability mass function, i.e. a normalized histogram. Use `plt.hist` on a list of page lengths, with the argument `density=True`. Other distribution plots are fine too, though.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:\n",
    "\n",
    "> First we will download the markup of each character's page and store it in the harddrive since there is not enough memory. The following cell tells the terminal to create the respective directories if the directories do not exist yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! [ ! -d ./villains ] && mkdir ./villains\n",
    "! [ ! -d ./heroes ] && mkdir ./heroes\n",
    "! [ ! -d ./ambiguous ] && mkdir ./ambiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The function `download_chars(group_set, directory_path)` downloads all of the specified group's character markup files from Wikipedia and into the specified directory path. A base query is modified for each character in the group to retrieve the markup and save as `directory_path/character_name.txt`. The function alerts that the downloading as finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def download_chars(group_set, directory_path):\n",
    "    \"\"\"downloads all of the group's characters markup into the specified relative subdirectory\n",
    "    \n",
    "    Args:\n",
    "        group_set: set containing the pageids of characters\n",
    "        directory_path: relative directory path to download files to\n",
    "    \"\"\"\n",
    "    \n",
    "    print('downloading %s' % directory_path, end='', flush=True)\n",
    "    \n",
    "    for char in group_set:\n",
    "        \n",
    "        query = 'https://www.wikipedia.org/w/api.php?action=query&format=json' \\\n",
    "                '&prop=revisions&rvprop=content&pageids=' + char\n",
    "\n",
    "        data = rq.get(query).json()\n",
    "        markup = data['query']['pages'][str(char)]['revisions'][0]['*']\n",
    "        \n",
    "        # replace slashes with underlines for file path consistency\n",
    "        title = re.sub('/','_',data['query']['pages'][str(char)]['title'])\n",
    "        \n",
    "        # save file as ./directory_path/file.txt\n",
    "        with open(directory_path + '/' + title + '.txt', 'w') as f: \n",
    "            f.write(markup)\n",
    "            \n",
    "        print('.', end='', flush=True)\n",
    "    \n",
    "    print('##### FINISHED DOWNLOADING %s #####' % directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"constant\" variables for our directories where files are stored since this will be referenced frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VILLAIN_DIR = 'villains'\n",
    "HERO_DIR = 'heroes'\n",
    "AMBIGUOUS_DIR = 'ambiguous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find sets of \"true\" villains and heroes - no ambiguous characters\n",
    "true_villains = villains_set - ambiguous_chars_set\n",
    "true_heroes = heroes_set - ambiguous_chars_set\n",
    "\n",
    "# download pages\n",
    "download_chars(true_villains, VILLAIN_DIR)\n",
    "download_chars(true_heroes, HERO_DIR)\n",
    "download_chars(ambiguous_chars_set, AMBIGUOUS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that we have all the markup files downloaded, we extract the lengths of the page of each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def extract_lengths(directory_path):\n",
    "    \"\"\"extracts the lengths of each character in the given directory path\n",
    "    \n",
    "    Args:\n",
    "        directory_path: path where the markup files are located\n",
    "        \n",
    "    Returns:\n",
    "        [int]: list of legnths of each character in the directory path\n",
    "    \"\"\"\n",
    "    \n",
    "    lengths = []\n",
    "    \n",
    "    # append lengths of each markup file\n",
    "    for char_path in listdir(directory_path + '/'): \n",
    "        with open(directory_path + '/' + char_path, 'r') as f:\n",
    "            lengths.append(len(f.read()))\n",
    "            \n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_villains = extract_lengths(VILLAIN_DIR)\n",
    "lengths_heroes = extract_lengths(HERO_DIR)\n",
    "lengths_ambiguous = extract_lengths(AMBIGUOUS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following cell plots the page length densities of each group of Marvel characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.title('Page length distribution of villains', fontsize=16)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.xlabel('Page legnth', fontsize=14)\n",
    "plt.hist(lengths_villains, bins=50, density=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(3,1,2)\n",
    "plt.title('Page length distribution of heroes', fontsize=16)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.xlabel('Page legnth', fontsize=14)\n",
    "plt.hist(lengths_heroes, bins=50, density=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(3,1,3)\n",
    "plt.title('Page length distribution of ambiguous characters', fontsize=16)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.xlabel('Page legnth', fontsize=14)\n",
    "plt.hist(lengths_ambiguous, bins=50, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the visualizations, it seems like the popularity is rather similar across all three groups. We can note that, by examining an estimate of how many characters have the shortest page lengths in each group, ambiguous characters seem the most popular, followed by heroes, and then villains. It seems in general, that there is more to discuss about ambiguous characters. This could be attributed to ambiguous character potentially causing more drama as they have dabbled in both sides of the MCU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.2**: Find the 10 characters from each class with the longest Wikipedia pages. Visualize their page lengths with bar charts. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:\n",
    "\n",
    "> The following cell defines `extract_char_length(directory_path)` which computes the page lengths of characters in the specified `directory_path`. It also defines the function `find_top_ten_chars(directory_path)` which finds the 10 characters from the given class (`directory_path`) with the longest Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_char_length(directory_path):\n",
    "    \"\"\"returns list of lengths of each characters and their page lengths\n",
    "    \n",
    "    Args:\n",
    "        directory_path: directory of the markup files\n",
    "    \n",
    "    Returns:\n",
    "        [[(str),(int)]]: list of characters and their respective page lengths\n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "    for character in listdir(directory_path + '/'):\n",
    "        with open(directory_path + '/' + character, 'r') as f:\n",
    "            char = character[:-4]\n",
    "            length = len(f.read())\n",
    "            lengths.append([char, length])\n",
    "    \n",
    "    return lengths\n",
    "\n",
    "\n",
    "def find_top_ten_chars(directory_path):\n",
    "    \"\"\"returns a list of the top ten characters and their page lengths\n",
    "    \n",
    "    Args:\n",
    "        directory_path: directory of the markup files\n",
    "        \n",
    "    Returns:\n",
    "        [[(str),(int)]]: list of top ten characters and their respective page lengths\n",
    "    \"\"\"\n",
    "    lengths = extract_char_length(directory_path)\n",
    "    lengths = np.array(lengths, dtype=object) # dtype=oject keeps our datatypes so int doesn't get cast to str literal\n",
    "    \n",
    "    # argsort by lengths\n",
    "    args = np.argsort(lengths[:,1], -1)[::-1]\n",
    "    \n",
    "    topten = lengths[args][:10]\n",
    "    \n",
    "    return topten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topten_villains = find_top_ten_chars(VILLAIN_DIR)\n",
    "topten_heroes = find_top_ten_chars(HERO_DIR)\n",
    "topten_ambiguous = find_top_ten_chars(AMBIGUOUS_DIR)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.subplot(3,1,1)\n",
    "plt.title('Top 10 Marvel supervillains page lengths', fontsize=20)\n",
    "plt.ylabel('Page length', fontsize=14)\n",
    "plt.xlabel('Supervillain', fontsize=14)\n",
    "plt.bar(topten_villains[:,0], topten_villains[:,1])\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.title('Top 10 Marvel superheroes page lengths', fontsize=20)\n",
    "plt.ylabel('Page length', fontsize=14)\n",
    "plt.xlabel('Superhero', fontsize=14)\n",
    "plt.bar(topten_heroes[:,0], topten_heroes[:,1])\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.title('Top 10 Marvel ambiguous characters page lengths', fontsize=20)\n",
    "plt.ylabel('Page length', fontsize=14)\n",
    "plt.xlabel('Ambiguous character', fontsize=14)\n",
    "plt.bar(topten_ambiguous[:,0], topten_ambiguous[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.3**: We are interested in knowing if there is a time-trend in the debut of characters.\n",
    "* Extract into three lists, debut years of heroes, villains, and ambiguous characters.\n",
    "* Do all pages have a debut year? Do some have multiple? How do you handle these inconsistencies?\n",
    "* For each class, visualize the amount of characters introduced over time. You choose how you want to visualize this data, but please comment on your choice. Also comment on the outcome of your analysis.\n",
    "\n",
    ">*Hint: The debut year is given on the debut row in the info table of a character's Wiki-page. There are many ways that you can extract this variable. You should try to have a go at it yourself, but if you are short on time, you can use this horribly ugly regular expression code:*\n",
    "\n",
    ">*`re.findall(r\"\\d{4}\\)\", re.findall(r\"debut.+?\\n\", markup_text)[0])[0][:-1]`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION: \n",
    "> The following cell defines the function `get_debut_years(directory_path)` which extracts the debut years of characters in the directory specified by the argument `directory_path`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_debut_years(directory_path):\n",
    "    \"\"\"returns a list of the debut years of characters in the specified directory path.\n",
    "    \n",
    "    Skips pages without (a) debut year(s). Takes the earliest year for pages with multiple debut years. \n",
    "    \n",
    "    Args:\n",
    "        directory_path: directory containing character markup files\n",
    "        \n",
    "    Returns:\n",
    "        [(int)]: list of debut years of characters in the specified directory in ints\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    debut_years = [] # list of debut years\n",
    "    \n",
    "    for character in listdir(directory_path + '/'):\n",
    "        with open(directory_path + '/' + character, 'r') as f:\n",
    "            markup = f.read()\n",
    "            debut_line = re.findall(r\"\\|debut.*\", markup) # find lines that contain the debut information\n",
    "            \n",
    "            if debut_line == []: # skip if no debut line(s)\n",
    "                continue\n",
    "            \n",
    "            debut_line = debut_line[0]\n",
    "            debut_year = re.findall(r\"\\d{4}\", debut_line) # extract the debut year(s)\n",
    "            \n",
    "            if debut_year == []: # skip if no date(s)\n",
    "                continue\n",
    "                \n",
    "            debut_year = np.min(np.array(debut_year, dtype=np.int)) # convert to ints and take the smallest debut year\n",
    "            debut_years.append(debut_year)\n",
    "    \n",
    "    return debut_years\n",
    "\n",
    "def get_all_debut_years():\n",
    "    \"\"\"returns three lists - debut yeras of supervillains, superheroes, and ambiguous characters\"\"\"\n",
    "    \n",
    "    return get_debut_years(VILLAIN_DIR), get_debut_years(HERO_DIR), get_debut_years(AMBIGUOUS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debut_years_villains, debut_years_heroes, debut_years_ambiguous = get_all_debut_years()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, we plot the total number of characters in the MCU by year for each group: supervillains, superheroes, and ambiguous characters. By plotting a line and the total number of characters, we get a better sense of the overall distribution of characters by group instead of yearly introductions of new characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(20,10), sharey=True)\n",
    "\n",
    "plt.title('Total number of characters in the Marvel Comic Universe by year, \\ncategorized by class role', fontsize=20)\n",
    "plt.xlabel('Year', fontsize=18)\n",
    "plt.ylabel('Number of characters', fontsize=18)\n",
    "\n",
    "\n",
    "\n",
    "ax.plot(np.sort(debut_years_villains), range(1,len(debut_years_villains)+1), label='Supervillains')\n",
    "ax.plot(np.sort(debut_years_heroes), range(1,len(debut_years_heroes)+1), label='Superheroes')\n",
    "ax.plot(np.sort(debut_years_ambiguous), range(1, len(debut_years_ambiguous)+1), label='Ambiguous characters')\n",
    "ax.set_xlim(1930, 2020)\n",
    "\n",
    "ax.legend(loc=2, fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the line for the supervillains continues from the left of the figure. This can be attributed to a  singlecharacter who debuted in the *year 1026*. An executive decision was made to exclude this single point from the plot since its inclusion does not add much in meaning to the visualization.\n",
    "It is interesting to see that initially there were more superheroes than both supervillains and ambiguous characters. This trend does not last for long as there is a significant increase in the number of supervillain introductions in the 60's. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
