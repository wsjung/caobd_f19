{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `caobd_s19` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Wednesday*, November 13, 2019, 23:59<br>\n",
    "**Peergrading deadline**: *Sunday*, November 17, 2019, 23:59<br>\n",
    "**Peergrading feedback deadline**: *Tuesday*, November 19, 2019, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.2.1**: Compute the sentiment score of each character's page and produce three histograms of sentiment scores, one for each faction. You can use the text strings you generated in the previous exercise. We will cheat a bit and use a library that does the scoring for us. Install `afinn` using `conda` or `pip` and extract the sentiment with that module. There's an example of how to use it on the library's [PyPi repository](https://pypi.python.org/pypi/afinn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Get a list of stopwords from nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#grabs words for character\n",
    "def get_clean_words(character_filename, faction):\n",
    "    def _isnum(w):\n",
    "        try:\n",
    "            int(w)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "        \n",
    "    # Load her markup\n",
    "    with open(\"./data/%s/%s\" % (faction, character_filename)) as fp:\n",
    "        markup = fp.read()\n",
    "\n",
    "    # Remove table and external links\n",
    "    markup_text = re.sub(r'\\{\\{[\\s\\S]*?\\}\\}', '', markup)\n",
    "\n",
    "    # Remove category links\n",
    "    markup_text = re.sub(r'\\[\\[Category.+\\]\\]', '', markup_text)\n",
    "\n",
    "    # Set words to lowercase and remove them if they are stop words\n",
    "    words = [w.lower() for w in re.findall('\\w+', markup_text) if w.lower() not in stopwords]\n",
    "\n",
    "    # Remove numbers\n",
    "    words = [w for w in words if not _isnum(w)]\n",
    "\n",
    "    return words\n",
    "\n",
    "# Example\n",
    "#get_clean_words(\"Iron Man.txt\", \"heroes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_marvel_text():\n",
    "    \n",
    "    faction_text = {'heroes': [], 'villains': [] , 'ambiguous': []}\n",
    "    \n",
    "    for faction in [\"heroes\", \"ambiguous\", \"villains\"]:\n",
    "        faction_chars = [c[:-4] for c in os.listdir(\"./data/%s\" % faction)]\n",
    "   \n",
    "        for char in faction_chars:\n",
    "            text_file = char+\".txt\"\n",
    "            \n",
    "            words = get_clean_words(text_file,faction)\n",
    "            \n",
    "            for w in words: \n",
    "                if w != char and w != 'ref':\n",
    "                    faction_text[faction].append(w)\n",
    "               \n",
    "    return faction_text\n",
    "        \n",
    "        \n",
    "faction_text = extract_marvel_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_marvel_text():\n",
    "    \n",
    "    faction_text = {'heroes': [], 'villains': [] , 'ambiguous': []}\n",
    "    \n",
    "    for faction in [\"heroes\", \"ambiguous\", \"villains\"]:\n",
    "        faction_chars = [c[:-4] for c in os.listdir(\"./data/%s\" % faction)]\n",
    "   \n",
    "        for char in faction_chars:\n",
    "            text_file = char+\".txt\"\n",
    "            \n",
    "            words = get_clean_words(text_file,faction)\n",
    "            \n",
    "            for w in words: \n",
    "                if w != char and w != 'ref':\n",
    "                    faction_text[faction].append(w)\n",
    "               \n",
    "    return faction_text\n",
    "        \n",
    "        \n",
    "faction_text = extract_marvel_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "\n",
    "hero_text = ' '.join(faction_text['heroes'])\n",
    "ambig_text = ' '.join(faction_text['ambiguous'])\n",
    "villain_text = ' '.join(faction_text['villains'])\n",
    "\n",
    "hero_score = afinn.score(hero_text)\n",
    "villain_score = afinn.score(villain_text)\n",
    "ambig_score = afinn.score(ambig_text)\n",
    "\n",
    "all_scores = [hero_score, villain_score, ambig_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18770.0\n",
      "-8475.0\n",
      "-2535.0\n"
     ]
    }
   ],
   "source": [
    "print(hero_score)\n",
    "print(villain_score)\n",
    "print(ambig_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 7.4.1**: Use any tool you like (you can do it manually, it's straight forward if you understand the method), to perform a TF-IDF transform on your BoW matrix from Ex. 7.3.1. The result should be a matrix of the same shape as the BoW, but with different values inside.\n",
    "1. Explain what these values mean.\n",
    "2. For the top three most written about characters in each class (so 9 in total), print out each of their 10 highest scoring words. Comment on any differences you observe in the type of words being used in different classes.\n",
    "\n",
    "> The TF value scores a word based on how often it appears within the current corpus (piece of text)\n",
    ">\n",
    "> The IDF value in conjunction with the TF value scores the importance of a word to a corpus compared to other corpuses. This often helps to find words that tell alot about a specific corpus since it rates a word highly if it appears alot within the specific text but not in others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for stemming words\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import words\n",
    "\n",
    "dictionary = words.words()\n",
    "\n",
    "#get list of unique words for matrix\n",
    "def construct_word_list():\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    wordSet = set([])\n",
    "    wordList = []\n",
    "    #special characters to ommit\n",
    "    string_check= re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "    \n",
    "    for faction in [\"heroes\", \"ambiguous\", \"villains\"]:\n",
    "        faction_chars = [c[:-4] for c in os.listdir(\"./data/%s\" % faction)]\n",
    "   \n",
    "        for char in faction_chars:\n",
    "            text_file = char+\".txt\"\n",
    "            \n",
    "            words = get_clean_words(text_file,faction)\n",
    "            \n",
    "            for w in words:\n",
    "                w = stemmer.stem(w)\n",
    "                #ommit stopwords, character names and words starting with numbers\n",
    "                if w != char and w != 'ref' and w not in stopwords and w[0] > 'A':\n",
    "                \n",
    "                    #check for special characters\n",
    "                    if(string_check.search(w) == None): \n",
    "                        wordSet.add(w)\n",
    "                        \n",
    "    wordSet = sorted(wordSet)\n",
    "    \n",
    "    for w in wordSet:\n",
    "        wordList.append(w)\n",
    "        \n",
    "    return wordList, wordSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return total list of chars\n",
    "def get_target_list():\n",
    "\n",
    "    target_list = []\n",
    "\n",
    "    for faction in ['heroes', 'villains', 'ambiguous']:\n",
    "        for char_filename in os.listdir('./data/%s/' % faction):\n",
    "            char_name = char_filename[:-4]\n",
    "            target_list.append([char_name, faction])\n",
    "\n",
    "    target_list.sort()\n",
    "\n",
    "    return target_list\n",
    "\n",
    "target_array = np.array(get_target_list())\n",
    "\n",
    "target_array_char = target_array[:,0] # list of chars\n",
    "target_array_faction = target_array[:,1] # list of faction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERY SLOW RUN TIME\n",
    "#creates unclean bow matrix\n",
    "def construct_bow(wordList, wordSet):\n",
    "    charList = []\n",
    "    stemmer = PorterStemmer()\n",
    "    matrix = np.empty(shape=(len(target_array_char), len(wordList)))\n",
    "    #special characters to ommit\n",
    "    string_check= re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for faction in [\"heroes\", \"ambiguous\", \"villains\"]:\n",
    "        faction_chars = [c[:-4] for c in os.listdir(\"./data/%s\" % faction)]\n",
    "   \n",
    "        for char in faction_chars:\n",
    "            text_file = char+\".txt\"\n",
    "            charList.append(char)\n",
    "            \n",
    "            row = np.zeros(len(wordList))\n",
    "            \n",
    "            words = get_clean_words(text_file,faction)\n",
    "            \n",
    "            \n",
    "            for w in words:\n",
    "                w = stemmer.stem(w)\n",
    "                #ommit stopwords, character names, and words starting with non letters\n",
    "                if w != char and w != 'ref' and w not in stopwords and w[0] >= 'A' :\n",
    "                    if(string_check.search(w) == None):\n",
    "                        index = wordList.index(w)\n",
    "                        row[index] += 1\n",
    "                    \n",
    "            matrix[i] = row\n",
    "            i += 1\n",
    "            \n",
    "    return charList, matrix       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletes single instance words from matrix\n",
    "def clean_bow(wordList, matrix):\n",
    "    delete_ind = []\n",
    "    sums = np.sum(matrix, axis=0)\n",
    "    ind = 0\n",
    "    \n",
    "    del_indices = []\n",
    "    \n",
    "    for i,n in enumerate(sums):\n",
    "        if n==1:\n",
    "            del_indices.append(i)\n",
    "            \n",
    "    matrix = np.delete(matrix, del_indices, axis=1)\n",
    "    wordList = np.array(wordList)\n",
    "    wordList = np.delete(wordList, del_indices)\n",
    "    \n",
    "    return matrix, wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the tf_idf matrix for a given BOW matrix\n",
    "def tf_idf(matrix):\n",
    "\n",
    "    def find_tf(matrix):\n",
    "        row_sums = matrix.sum(axis=1)\n",
    "        tf = matrix / row_sums[:, np.newaxis]\n",
    "        return tf\n",
    "\n",
    "    def find_idf(matrix):\n",
    "        num_rows = len(matrix)\n",
    "        idf = []\n",
    "\n",
    "        for word in matrix.T:\n",
    "            f = 0\n",
    "            for x in word:\n",
    "                if x > 0:\n",
    "                    f += 1\n",
    "            idf.append(num_rows / f)\n",
    "\n",
    "        return np.array(idf)\n",
    "\n",
    "\n",
    "    # part 1: normalize rows\n",
    "    tf = find_tf(matrix)\n",
    "\n",
    "    # part 2: compute idf of each word\n",
    "    idf = find_idf(matrix)\n",
    "\n",
    "    # part 3: compute tf-idf\n",
    "\n",
    "    matrix_tfidf = np.empty(matrix.shape)\n",
    "    for i in range(matrix_tfidf.shape[0]):\n",
    "        for j in range(matrix_tfidf.shape[1]):\n",
    "            matrix_tfidf[i][j] = tf[i,j] * idf[j]\n",
    "\n",
    "    return matrix_tfidf, tf, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts num of words for each character and its index\n",
    "def char_word_list():\n",
    "    word_count = []\n",
    "    faction_ind = []\n",
    "    stemmer = PorterStemmer()\n",
    "    i = 0\n",
    "    \n",
    "    #special characters to ommit\n",
    "    string_check= re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "\n",
    "    for faction in [\"heroes\", \"ambiguous\", \"villains\"]:\n",
    "        faction_chars = [c[:-4] for c in os.listdir(\"./data/%s\" % faction)]\n",
    "        faction_ind.append(i)\n",
    "   \n",
    "        for char in faction_chars:\n",
    "            text_file = char+\".txt\"\n",
    "            \n",
    "            count = 0\n",
    "            words = get_clean_words(text_file,faction)\n",
    "            \n",
    "            for w in words:\n",
    "                w = stemmer.stem(w)\n",
    "                #ommit stopwords, character names, and words starting with non letters\n",
    "                if w != char and w != 'ref' and w not in stopwords and w[0] >= 'A' :\n",
    "                    if(string_check.search(w) == None):\n",
    "                        count += 1\n",
    "            word_count.append((count,char, i))\n",
    "            i += 1\n",
    "            \n",
    "    return word_count, faction_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorts word count of each factions characters\n",
    "def highest_word_count(word_count, faction_ind):\n",
    "    hero_ind = faction_ind[0]\n",
    "    ambig_ind = faction_ind[1]\n",
    "    villain_ind = faction_ind[2]\n",
    "    \n",
    "    hero = word_count[hero_ind : ambig_ind-1]\n",
    "    ambig = word_count[ambig_ind : villain_ind-1]\n",
    "    villain = word_count[villain_ind :]\n",
    "    \n",
    "    hero.sort(key=lambda tup: tup[0], reverse = True)\n",
    "    ambig.sort(key=lambda tup: tup[0], reverse = True)\n",
    "    villain.sort(key=lambda tup: tup[0], reverse = True)\n",
    "    \n",
    "    return hero, ambig, villain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab index of highest wrank words for given character index\n",
    "def highest_rank_words(char_ind, matrix_tfidf):\n",
    "    row = matrix_tfidf[char_ind]\n",
    "    \n",
    "    tmp = np.argsort(row)\n",
    "    tmp = tmp[::-1]\n",
    "\n",
    "    inds = tmp[:10]\n",
    "#     print(row[inds])\n",
    "    \n",
    "    return inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the highest scored words for each faction chars in tfidf matrix\n",
    "def print_highest_score_words(h_word, a_word, v_word, matrix_tfidf, word_list):\n",
    "    print(\"HEROS\")\n",
    "    h = h_word[:3]\n",
    "    for c in h:\n",
    "        words = []\n",
    "        ind = highest_rank_words(c[2],matrix_tfidf)\n",
    "        for i in ind:\n",
    "            words.append(word_list[i])\n",
    "        print(c[1],\":\",words,\"\\n\")\n",
    "        \n",
    "    print(\"AMBIG\")\n",
    "    a = a_word[:3]\n",
    "    for c in a:\n",
    "        words = []\n",
    "        ind = highest_rank_words(c[2],matrix_tfidf)\n",
    "        for i in ind:\n",
    "            words.append(word_list[i])\n",
    "        print(c[1],\":\",words,\"\\n\")\n",
    "        \n",
    "    print(\"VILLAIN\")\n",
    "    v = v_word[:3]\n",
    "    for c in v:\n",
    "        words = []\n",
    "        ind = highest_rank_words(c[2],matrix_tfidf)\n",
    "        for i in ind:\n",
    "            words.append(word_list[i])\n",
    "        print(c[1],\":\",words,\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING, SLOW RUN TIME\n",
    "wordList, wordSet = construct_word_list()\n",
    "charList, matrix = construct_bow(wordList, wordSet)\n",
    "clean_matrix, clean_wordList = clean_bow(wordList, matrix)\n",
    "matrix_tfidf, tf, idf = tf_idf(clean_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEROS\n",
      "Psylocke : ['new11', 'issue2', 'issue257', 'betsi', 'slaymast', 'kwannon', 'issue455', 'issue77', 'm2000', 'babe'] \n",
      "\n",
      "Cyclops (Marvel Comics) : ['uxm511', 'doel', 'gigawatt', 'eyewear', 'uxm504', 'dryad', 'quartz', 'cyclop', 'jeer', 'shiv'] \n",
      "\n",
      "Spider-Man : ['saffel', 'saffel21', 'af', 'saffel124', 'maci', 'kindersly4', 'reelzchannel', 'venom2', 'gcd3', 'kempton'] \n",
      "\n",
      "AMBIG\n",
      "Phoenix Force (comics) : ['vel', 'phoenix', 'avx9', 'avx11', 'darkseid', 'avx8', 'avx5', 'stormphoenix', 'reunifi', 'giraud'] \n",
      "\n",
      "Emma Frost : ['uxm', 'xmv2', 'xm', 'astrid', 'emmelin', 'nxmv2', 'xx', 'emmafrostfil', 'cw3', 'nxm'] \n",
      "\n",
      "Sabretooth (comics) : ['sangr', 'goda', 'schreiber', 'sabretooth', 'liev', 'clara', 'saul', 'birdi', 'creed', 'fuego'] \n",
      "\n",
      "VILLAIN\n",
      "Doctor Octopus : ['tvodo', 'torbert', 'octaviu', 'smugli', 'rosi', 'schenectadi', 'lamaz', 'carlyl', 'octavius', 'ultimatespiderman'] \n",
      "\n",
      "Loki (comics) : ['veriti', 'hiddleston', 'ikol', 'loki', 'gunnar', 'sigyn', 'cbrsiege3', 'var', 'helhorn', 'eldr'] \n",
      "\n",
      "Vulture (Marvel Comics) : ['shallot', 'toom', 'bestman', 'clifton', 'vulturion', 'natal', 'vultur', 'honcho', 'christin', 'toomeston'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_count, faction_ind = char_word_list()\n",
    "h_word, a_word, v_word = highest_word_count(word_count, faction_ind)\n",
    "print_highest_score_words(h_word, a_word, v_word, matrix_tfidf, clean_wordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There doesn't seem to be specific differences between each faction, but each character seems to have its own unique words. For instance, Psylocke, Cyclops, and Emma Frost have words that reference specific comic issues. Cyclops' words in particular seem to be very specific to him, mentioning eyewear. For Loki we can see some nordic sounding places such as var, helhorn and even his name backwards as ikol. Most of this things may have to do with the preprossesing of the words used in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 8.2.1**: Modify the script from Ex. 8.1.1 so that it instead of word counts outputs the number of characters, words and lines in the file. Post as your answer in two seperate cells, (1) the code in the script in a code cell, and (2) the terminal output in a markdown cell with the text indented by one tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from mrjob.job import MRJob\n",
    "\n",
    "    class MRWordCounter(MRJob):  \n",
    "\n",
    "        def mapper(self, _, line):\n",
    "            yield \"chars\", len(line)\n",
    "\n",
    "            for words in line.split():\n",
    "                yield \"words\", 1\n",
    "\n",
    "            yield \"lines\", 1\n",
    "    \t\n",
    "\n",
    "        def reducer(self, key, values):\n",
    "            yield key, sum(values)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        MRWordCounter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Running step 1 of 1...\n",
    "    job output is in /tmp/MapReduce2.USER.20191105.143957.822594/output\n",
    "    Streaming final output from /tmp/MapReduce2.USER.20191105.143957.822594/output...\n",
    "    \"lines\"\t4\n",
    "    \"words\"\t21\n",
    "    \"chars\"\t88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 8.2.3**: Lets go a bit deeper. In this exercise you will implement a MapReduce-MapReduce operation, which computes the same thing as we computed above, but takes as input friend-network data in a slightly more common format: \n",
    "\n",
    ">        A B\n",
    ">        A C\n",
    ">        A D\n",
    ">        B C\n",
    ">        B D\n",
    ">        B E\n",
    ">        C D\n",
    ">        C E\n",
    ">        D E\n",
    "\n",
    ">Each line is a \"friend-link\". The links are undirected and each only occurs once.\n",
    "\n",
    ">Your job now, is to produce the same output as you did in Ex. 8.2.2, using this input data. To get started faster, use the template below, which shows how to chain together multiple MapReduce steps. Fill out the template and show the output that you get from the terminal when you run it. Clarify whether it corresponds with the output from Ex. 8.2.2.\n",
    "\n",
    ">*Hint: Try to write the first MapReduce step such that it outputs key-value pairs that correspond to the input data format from Ex. 8.2.2. Then you can reuse your solution to Ex. 8.2.2 in your second MapReduce step.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 8.2.4**: We can go even further! Let's add a third MapReduce step to count the number of triangles in a network. Again use this input data:\n",
    "\n",
    ">        A B\n",
    ">        A C\n",
    ">        A D\n",
    ">        B C\n",
    ">        B D\n",
    ">        B E\n",
    ">        C D\n",
    ">        C E\n",
    ">        D E\n",
    "\n",
    ">to validate that your implementation works. It should produce 7 triangles.\n",
    "\n",
    ">1. Now compute the number of triangles in [this file](http://snap.stanford.edu/data/facebook_combined.txt.gz) which contains 88234 links in an anonymized facebook network. Don't print the whole output, just report the number you get.\n",
    ">2. Do the same instead using all 2766607 road segments in California as your input. Go to [this site](https://www.cise.ufl.edu/research/sparse/matrices/SNAP/roadNet-CA.html) and download the data in Matrix Market format (`.mtx`). Unzip the file and remove the first 50 lines from it, since that is just markup that we don't need. The file is pretty big so you can expect it to take some time (~4 minutes on my computer). Report the number you get.\n",
    "\n",
    ">*Hint: Counting triangles is equivalent to counting \"common friends\". One way to do that is to just count the collective number of common friends that exist in a network. Depending on your implementation you might want to correct your result by a factor 3, since it is likely that you end up counting each triangle three times (one for each point in it).*\n",
    "\n",
    ">*Nerdy sidenote: Why would anyone want to count triangles??? Well, in network science there is a lot of statistical measures that include the count of triangles in a network. For example, the [clustering coefficient](https://en.wikipedia.org/wiki/Clustering_coefficient), which reveals the proportion of small closed loops in a network, is computed as the number of realized triangles divided by the number of possible triangles.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
