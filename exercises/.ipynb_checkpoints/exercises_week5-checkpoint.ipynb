{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Machine Learning 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "There are innumerable machine learning models (and algorithms for fitting them to data) out there, and each one has something special about it that makes it suitable to a specific type of problem. To apply machine learning and get some initial results is fairly straight forward. Getting under the hood, however, requires a bit of work. This week we will look into how Decision Trees and PCA works. In the exercises today you will:\n",
    "\n",
    "* Implement a standard decision tree mechanism\n",
    "* Play around with a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get started**: Load the data you used last week to classify hero/villainness. It should consist of two arrays, one of dimensions ($N_{characters}$, $N_{teams}$) which is your feature matrix that has the team alliances of each character as a row vector of ones and zeros, and another of dimensions ($N_{characters}$, ) which is your target array that gives whether a character is a hero (1) or a villain (0). You can either load the data or copy/paste the code that generates it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VILLAIN_DIR = 'villains'\n",
    "HERO_DIR = 'heroes'\n",
    "AMBIGUOUS_DIR = 'ambiguous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alliances(char, faction=None):\n",
    "    \"\"\"Return list of alliances for Marvel character.\"\"\"\n",
    "    \n",
    "    if faction is None:\n",
    "        for faction in [\"heroes\", \"ambiguous\", \"villains\"]:\n",
    "            faction_chars = [c[:-4] for c in os.listdir(\"../data/%s\" % faction)]\n",
    "            if char in faction_chars:\n",
    "                break\n",
    "    \n",
    "    # Load character markup\n",
    "    with open(\"../data/%s/%s.txt\" % (faction, char)) as fp:\n",
    "        markup = fp.read()\n",
    "\n",
    "    # Get alliance field\n",
    "    alliances_field = re.findall(r\"alliances[\\w\\W]+?\\|.+=\", markup)\n",
    "    if alliances_field == []:\n",
    "        return []\n",
    "\n",
    "    # Extract teams from alliance field\n",
    "    return list(set([t[2:-1] for t in re.findall(r\"\\[\\[.+?[\\]\\|]\", alliances_field[0][10:])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_alliances():\n",
    "    \n",
    "    teams = set([])\n",
    "    \n",
    "    for faction in [\"heroes\", \"ambiguous\", \"villains\"]: # for each faction\n",
    "        for char in os.listdir(\"../data/%s\" % faction): # for each character in faction\n",
    "            with open(\"../data/%s/%s\" % (faction, char)) as f:\n",
    "                markup = f.read()\n",
    "                \n",
    "                # find alliances\n",
    "                alliances_field = re.findall(r\"alliances[\\w\\W]+?\\|.+=\", markup)\n",
    "                \n",
    "                if alliances_field != []:\n",
    "                    [teams.add(t[2:-1]) for t in re.findall(r\"\\[\\[.+?[\\]\\|]\", alliances_field[0][10:])]\n",
    "    \n",
    "    teams = list(teams)\n",
    "    teams.sort()\n",
    "    \n",
    "    return np.array(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teams_vector(char, faction=None, verbose=True):\n",
    "    char_alliances = get_alliances(char)\n",
    "    tmp = [1 if team in char_alliances else 0 for team in all_teams]\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(sum(tmp))\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_alliance_matrix():\n",
    "    targ_array = []\n",
    "    sort_vec = []\n",
    "    team_matrix = []\n",
    "    \n",
    "    for faction, val in [[\"heroes\", 1], [\"villains\",0]]: # for each faction\n",
    "        for char in os.listdir(\"../data/%s\" % faction): # for each character in faction\n",
    "            char = char[:-4] # remove .txt file extension\n",
    "            \n",
    "            team_vec = get_teams_vector(char, verbose=False)\n",
    "            if sum(team_vec) != 0:\n",
    "                team_matrix.append(team_vec)\n",
    "                targ_array.append([char, val]) # target array\n",
    "#                 sort_vec.append([char, val])\n",
    "            \n",
    "            \n",
    "    # sort the matrix by character name\n",
    "    team_matrix = np.matrix(team_matrix)\n",
    "    team_matrix = team_matrix[np.argsort(targ_array, axis=0)[:,0]]\n",
    "    \n",
    "    # sort target array\n",
    "    targ_array.sort()\n",
    "    \n",
    "    return targ_array, team_matrix, sort_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teams = get_all_alliances()\n",
    "target_array, team_matrix, sort_vec = get_alliance_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement the decision making algorithm of a decision tree classifier, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.1.1**: Read about [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)).\n",
    "1. What is it? How is it defined mathematically (write out the formula in LateX formatting)?\n",
    "2. Write a function that computes the Shannon-entropy of a probability vector. Compute the Shannon entropy of `p=[0.4, 0.6]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION\n",
    "\n",
    "1. Shannon entropy is a way to measure diversity that is characterized by the definitions of entropy satisfying the following form:\n",
    "\n",
    "$$ - \\sum_{i=1}^{n} p_i \\log_b (p_i),$$\n",
    "where $K$ is a constant corresponding to a choice of measurement units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_shannon_entropy(pdf):\n",
    "    \n",
    "    entropy = 0\n",
    "    \n",
    "    for p in pdf:\n",
    "        entropy += p * np.log2(p)\n",
    "        \n",
    "    return -1*entropy\n",
    "\n",
    "p = [0.4, 0.6]\n",
    "compute_shannon_entropy(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.1.2**: Split your data into two subsets. One where characters are affiliated with X-men and one where they are not.\n",
    "1. What is the entropy of target labels in each subset?\n",
    "2. What is the weighted average entropy of the split?\n",
    "3. Write a function that computes the weighted average entropy of a split, given the data and team (name or id) on which to split the data.\n",
    "4. Plot the distribution of split entropy for all features. Comment on the result. My figure looks [like this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_6.2.2.4.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_teams)==team_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[590, 'X-Men'], [591, 'X-Men (comics)']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[i,team] for i,team in enumerate(all_teams) if team=='X-Men' or team=='X-Men (comics)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(team_matrix[:, 590]) # number of characters in 'X-Men'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_men_indices = [i for i,char in enumerate(team_matrix[:, 590]) if char.item(0)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_x_men_indices = np.arange(len(target_array))\n",
    "non_x_men_indices = np.delete(non_x_men_indices, x_men_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_men = np.array(target_array)[x_men_indices]\n",
    "#x_men[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_x_men = np.array(target_array)[non_x_men_indices]\n",
    "#non_x_men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pmf(arr):\n",
    "    labels = arr[:,1]\n",
    "    \n",
    "    val, count = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    return count / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54410307, 0.45589693])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmf_x_men = find_pmf(x_men)\n",
    "pmf_non_x_men = find_pmf(non_x_men)\n",
    "\n",
    "pmf_x_men\n",
    "pmf_non_x_men"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy of X-Men subset: 0.403797\n",
      "entropy of non X-Men subset: 0.994380\n"
     ]
    }
   ],
   "source": [
    "entropy_x_men = compute_shannon_entropy(pmf_x_men)\n",
    "entropy_non_x_men = compute_shannon_entropy(pmf_non_x_men)\n",
    "\n",
    "print('entropy of X-Men subset: %.6f' % entropy_x_men)\n",
    "print('entropy of non X-Men subset: %.6f' % entropy_non_x_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted average entropy of the split: 0.947500\n"
     ]
    }
   ],
   "source": [
    "total = len(target_array)\n",
    "weighted_avg_entropy = (len(x_men) / total) * entropy_x_men + (len(non_x_men) / total) * entropy_non_x_men\n",
    "print('weighted average entropy of the split: %.6f' % weighted_avg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_team(list_teams, char_array, matrix, team):\n",
    "    \n",
    "    assert len(list_teams)==matrix.shape[1]\n",
    "    \n",
    "    # find index of team\n",
    "    index_team = np.where(list_teams==team)\n",
    "    \n",
    "    # indices of characters in team\n",
    "    member_indices = [i for i,char in enumerate(matrix[:, index_team]) if char.item(0)==1]\n",
    "    # indices of non-members\n",
    "    nonmember_indices = np.arange(len(char_array))\n",
    "    nonmember_indices = np.delete(nonmember_indices, member_indices)\n",
    "    \n",
    "    # list of members and nonmembers\n",
    "    team_members = np.array(char_array)[member_indices]\n",
    "    team_nonmembers = np.array(char_array)[nonmember_indices]\n",
    "    \n",
    "    return team_members, team_nonmembers, member_indices, nonmember_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_list_entropy(list_team):\n",
    "    return compute_shannon_entropy(find_pmf(list_team))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_split_weighted_avg_entropy(list_teams, char_array, matrix, team):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        list_teams: list of all teams in alphabetical order\n",
    "        char_array: list of characters and whether they are a hero (1) or villain (0)\n",
    "        matrix: matrix of characters and their association with a team\n",
    "        team: team to split data on \n",
    "    \"\"\"\n",
    "    \n",
    "    team_members, team_nonmembers, member_indices, nonmember_indices = split_by_team(list_teams, char_array, matrix, team)\n",
    "    \n",
    "    entropy_members = compute_list_entropy(team_members)\n",
    "    entropy_nonmembers = compute_list_entropy(team_nonmembers)\n",
    "    \n",
    "    # split entropy\n",
    "    weighted_avg_entropy = (len(team_members) / len(char_array)) * entropy_members + (len(team_nonmembers) / len(char_array)) * entropy_nonmembers\n",
    "    \n",
    "    return weighted_avg_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475001439630428"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_split_weighted_avg_entropy(all_teams, target_array, team_matrix, 'X-Men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_weighted_avgs(list_teams, char_array, matrix):\n",
    "    weighted_avgs = []\n",
    "    for team in all_teams:\n",
    "        weighted_avgs.append(compute_split_weighted_avg_entropy(list_teams, char_array, matrix, team))\n",
    "    return weighted_avgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_avgs = compute_all_weighted_avgs(all_teams)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(weighted_avgs)\n",
    "plt.title('Distribution of split entropies for all features')\n",
    "plt.xlabel('Split entropy')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.1.3**: Print the maximum entropy path of a decision tree.\n",
    ">\n",
    ">1. Implement the following pseudocode and print the output:<br><br>\n",
    ">Step 1. Find `team` that gives lowest split entropy for `data`. Print `team`.<br>\n",
    ">Step 2. Split `data` on `team`, to produce `data0` and `data1`. Print the entropy of each, as well as their weighted avg. entropy.<br>\n",
    ">Step 3. Overwrite the `data` variable with either `data0` or `data1`, depending on which has the highest entropy.<br>\n",
    ">Step 4. Stop if there are less than 5 datapoints in `data`. Otherwise start over from 1.<br><br>\n",
    ">My output looks [like this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_6.2.3.1.png) for the first five splits.<br><br>\n",
    ">\n",
    ">2. Comment on decision path your code takes: How splits are there? Do you notice anything interesting about the final splits? Why do we choose to stop splitting before `data` get smaller than 5?\n",
    ">3. Train a `sklearn.tree.DecisionTreeClassifier` classifier on the dataset. Initiate the classifier with `criterion='entropy'`. What are the most important features of this classifier? How does this line up with the order of the order of splits you just printed (a comment is fine)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Abigail Brand', '1'],\n",
       "       ['Abomination (comics)', '0'],\n",
       "       ['Abraham Cornelius', '0'],\n",
       "       ...,\n",
       "       ['Yukio (comics)', '1'],\n",
       "       ['Zeke Stane', '0'],\n",
       "       ['Zzzax', '0']], dtype='<U38')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 1096\n",
      "list_teams size: 622\n",
      "matrix shape: 1096 x 622\n",
      "Split 0 : Avengers (comics)\n",
      "--------------------------\n",
      "\tdata0:\n",
      "\t\tsize:\t92\n",
      "\t\tentropy:\t0.304587\n",
      "\tdata1:\n",
      "\t\tsize:\t1004\n",
      "\t\tentropy:\t0.993116\n",
      "\t--> average entropy: 0.935320\n",
      "\n",
      "data size: 1004\n",
      "list_teams size: 621\n",
      "matrix shape: 1004 x 621\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-41c405af11cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mweighted_avgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_all_weighted_avgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_teams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmin_entropy_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_avgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mteam_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_teams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin_entropy_arg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-238-f7d8775f9c42>\u001b[0m in \u001b[0;36mcompute_all_weighted_avgs\u001b[0;34m(list_teams, char_array, matrix)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mweighted_avgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mteam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_teams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mweighted_avgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_split_weighted_avg_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_teams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweighted_avgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-227-8a74b0bf816f>\u001b[0m in \u001b[0;36mcompute_split_weighted_avg_entropy\u001b[0;34m(list_teams, char_array, matrix, team)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mteam_members\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteam_nonmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmember_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonmember_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_by_team\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_teams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mentropy_members\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_list_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteam_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-225-02bd4fb368c0>\u001b[0m in \u001b[0;36msplit_by_team\u001b[0;34m(list_teams, char_array, matrix, team)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# indices of characters in team\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmember_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_team\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# indices of non-members\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnonmember_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-225-02bd4fb368c0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# indices of characters in team\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmember_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_team\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# indices of non-members\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnonmember_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for size 0"
     ]
    }
   ],
   "source": [
    "data = target_array\n",
    "list_teams = all_teams\n",
    "matrix = team_matrix\n",
    "\n",
    "split_iter = 0\n",
    "\n",
    "print('data size: %d' % len(data))\n",
    "print('list_teams size: %d' % len(list_teams))\n",
    "print('matrix shape: %d x %d' % (matrix.shape))\n",
    "\n",
    "while len(data) >= 5:\n",
    "    \n",
    "    weighted_avgs = compute_all_weighted_avgs(list_teams, data, matrix)\n",
    "    min_entropy_arg = np.argmin(weighted_avgs)\n",
    "    team_name = all_teams[min_entropy_arg]\n",
    "    print('Split %d : %s' % (split_iter, team_name))\n",
    "    print('-' * (9 + len(team_name)))\n",
    "    \n",
    "    split_iter += 1\n",
    "\n",
    "    # split by lowest entropy team\n",
    "    data0, data1, data0_indices, data1_indices = split_by_team(list_teams, data, matrix, team_name)\n",
    "    \n",
    "    entropy_data0 = compute_list_entropy(data0)\n",
    "    entropy_data1 = compute_list_entropy(data1)\n",
    "    weighted_avg_entropy = weighted_avgs[min_entropy_arg]\n",
    "\n",
    "    print('\\tdata0:\\n\\t\\tsize:\\t%d\\n\\t\\tentropy:\\t%f' % (len(data0), entropy_data0))\n",
    "    print('\\tdata1:\\n\\t\\tsize:\\t%d\\n\\t\\tentropy:\\t%f' % (len(data1), entropy_data1))\n",
    "    print('\\t--> average entropy: %f\\n' % weighted_avg_entropy)\n",
    "    \n",
    "    if entropy_data0 > entropy_data1:\n",
    "        delete_indices = data1_indices\n",
    "        data = data0\n",
    "    else:\n",
    "        delete_indices = data0_indices\n",
    "        data = data1        \n",
    "    \n",
    "    # remove current team from list_teams\n",
    "    list_teams = np.delete(list_teams, np.argwhere(list_teams==team_name))\n",
    "    \n",
    "    # remove the char entries from matrix\n",
    "    matrix = np.delete(matrix, delete_indices, axis=0)\n",
    "    \n",
    "    # remove team entry from matrix\n",
    "    matrix = np.delete(matrix, min_entropy_arg, axis=1)\n",
    "    \n",
    "    print('data size: %d' % len(data))\n",
    "    print('list_teams size: %d' % len(list_teams))\n",
    "    print('matrix shape: %d x %d' % (matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regressions are great baseline models for comparing how well a more complicated model works.\n",
    "They are literally just linear regressions where the output is *squeezed* through a `sigmoid` function,\n",
    "so the returned value is between 0 and 1 (which can be interpreted as a probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.2.1**: Implement a logistic regression model.\n",
    "Below I have implemented a *linear* regression model which takes *two* input parameter.\n",
    "Create another function, `logistic_regression` that takes again two input\n",
    "variables and returns a value between 0 and 1. Demonstrate that it works by inputting\n",
    "the data, `x=[1, 1]`, and parameters, `w0=1`, `w1=1` and `w2=0`, below, and show that it gives 0.88.\n",
    ">\n",
    ">*Hint*: The `sigmoid` function can look like this:\n",
    ">\n",
    ">        def sigmoid(x):\n",
    ">            return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T07:10:28.534347Z",
     "start_time": "2019-09-29T07:10:28.525702Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression(x, w0, w1, w2):\n",
    "    return x[0] * w0 + x[1] * w1 + w2\n",
    "\n",
    "# example\n",
    "x = [1, 1]\n",
    "w0 = 1; w1 = 1; w2 = 0\n",
    "linear_regression(x, w0, w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.2.2**: *Fit* a logistic regression! You can use the `scipy` module `scipy.optimize.curve_fit`\n",
    "to fit a model to some data (i.e. find the best parameter values `w`). Below I have implemented an\n",
    "example of how to fit some data to the linear regression above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T07:15:45.099126Z",
     "start_time": "2019-09-29T07:15:45.087872Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "def generate_X_linear(N=200):\n",
    "    \"\"\"A little function that creates some data.\"\"\"\n",
    "    x = np.vstack([\n",
    "        np.random.normal([-2, -2], 1, size=(int(N/2), 2)),\n",
    "        np.random.normal([2, 2], 1, size=(int(N/2), 2))\n",
    "    ])\n",
    "\n",
    "    y = np.array([0] * int(N/2) + [1] * int(N/2))\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Generate input and output data\n",
    "x, y = generate_X_linear()\n",
    "\n",
    "optimal_params, cov_params = curve_fit(linear_regression, x.T, y)\n",
    "optimal_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use the `logistic_regression` function you wrote in the previous exercise and fit it to `x` and `y`.\n",
    "> Store the optimal parameters in a variable called `optimal_params` (like above), \n",
    "> then run the code cell below to plot the predictions. Mine looks like [this](https://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_5.5.2.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T07:36:06.484274Z",
     "start_time": "2019-09-29T07:36:06.304742Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.scatter(x[:, 0], x[:, 1], c=[logistic_regression(x_, *optimal_params) for x_ in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 (extra): Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These days, neural networks are probably the hottest item in machine learning, and for good reason. Neural networks *can* be a bit of a mouthful to understand, and since I didn't talk about them in this course much (if at all), you can solve this problem if you have sufficient interest for it. That said, it is a skill well worth investing in.\n",
    "\n",
    "We will be using the deep learning library **PyTorch** to build some neural networks with which we can play around with. *Nerdnote: Why not something more high-level such as Keras with a Tensorflow backend? Well PyTorch is easier to install. And using it, it's clearer what actually happens when you fit a neural network. Finally, for those taking the ANN course it's good to see something new.*\n",
    "\n",
    "To get torch running you need to first install it. It's should be fairly straight forward, but depending on\n",
    "your machine you may have to run different commands to install it. Check out the installation guide [here](https://pytorch.org/).\n",
    "\n",
    "Once you've installed it you should be able to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:42:50.639887Z",
     "start_time": "2019-02-21T11:42:50.342085Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No errors? Great! Then let's make a small neural network that we know all to well at this point, and see if we can classify points with it. First, we generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:43:20.228851Z",
     "start_time": "2019-02-21T11:43:20.220487Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_X_nonlinear(N=200, R=5):\n",
    "\n",
    "    X_inner = torch.randn(int(N/2), 2)\n",
    "\n",
    "    X_outer = torch.tensor([\n",
    "        [R*np.cos(theta), R*np.sin(theta)]\n",
    "        for theta in np.linspace(0, 2 * np.pi, int(N/2))\n",
    "    ]) + torch.randn(int(N/2), 2)\n",
    "\n",
    "    X = torch.cat([X_inner, X_outer], dim=0)\n",
    "   \n",
    "    y = torch.cat([\n",
    "        torch.zeros(int(N/2)).reshape(-1, 1),\n",
    "        torch.ones(int(N/2)).reshape(-1, 1)\n",
    "    ])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Number of training datapoints\n",
    "N = 500\n",
    "\n",
    "# Generate the data (note that code is using torch arrays now)\n",
    "x, y = generate_X_nonlinear(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this data, we can now set up a neural network and train it. We are not going to do anything fancy here, just make a simple 2-layer feed forward neural network with 2 input neurons, 3 hidden neurons and 1 output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:45:30.703601Z",
     "start_time": "2019-02-21T11:45:30.700648Z"
    }
   },
   "outputs": [],
   "source": [
    "# The layers and their number of neurons\n",
    "sizes = [2, 3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to define the model. PyTorch has an API called `Sequential` that makes this pretty easy. Try to get an idea of what the below code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:46:28.285802Z",
     "start_time": "2019-02-21T11:46:28.280709Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(sizes[0], sizes[1]),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(sizes[1], sizes[2]),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to define a loss function. We are just going to use the sum of squared errors, and again PyTorch has got an implementation we can pick right off the shelf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:47:27.807695Z",
     "start_time": "2019-02-21T11:47:27.804668Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can just train it! We pick a learning rate parameter (which is how big the steps we take during\n",
    "gradient descent are), define an *optimizer* which is a wrapper for training that abstracts away all the\n",
    "usual steps. The `epochs` are simply the number of times we train on the entire dataset. Then we are ready to\n",
    "train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:52:35.129417Z",
     "start_time": "2019-02-21T11:52:34.896992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learning_rate = 1e-1\n",
    "epochs = 100\n",
    "mini_batch_size = 100\n",
    "\n",
    "# Optimization wrapper\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train\n",
    "for t in range(epochs):\n",
    "    \n",
    "    # Randomly permute the row indices to get something like:\n",
    "    # tensor([16214, 18491, 16308,  ..., 19629, 17565, 24696])\n",
    "    permutation = torch.randperm(x.size()[0])\n",
    "    \n",
    "    # Start looping over the mini-batches! Each index `k` is\n",
    "    # `mini_batch_size` values apart.\n",
    "    for k in np.arange(0, x.size()[0], mini_batch_size):\n",
    "        \n",
    "        # Extract mini-batch data. The rest is the same\n",
    "        mini_batch_indices = permutation[k:k+mini_batch_size]\n",
    "        x_ = x[mini_batch_indices, :]\n",
    "        y_ = y[mini_batch_indices, :]\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(x_)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, y_)\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers (i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    # Print progress (here evaluating on all the data so we can compare)\n",
    "    if t % 10 == 0:\n",
    "        loss = loss_fn(model(x), y)\n",
    "        print(t, \"train:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the errors we are showing here are the sum of squared errors. You'd have to jump through a small hoop (which is very doable) to get the accuracy. Also, this is the error on the training data. So there's no guarantee that we don't overfit here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can visualize the predictions next to the true labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:53:44.190958Z",
     "start_time": "2019-02-21T11:53:43.877249Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "class cmap_in_range:\n",
    "    \"\"\"Create map to range of colors inside given domain.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> cmap = cmap_in_range([0, 1])\n",
    "    >>> cmap(0.1)\n",
    "    (0.30392156862745101, 0.30315267411304353, 0.98816547208125938, 1.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, cmap_domain, cmap_range=[0, 1], cmap_style='rainbow'):\n",
    "        self.cmap_domain = cmap_domain\n",
    "        self.cmap_range = cmap_range\n",
    "        self.m = interp1d(cmap_domain, cmap_range)\n",
    "        self.cmap = plt.get_cmap(cmap_style)\n",
    "        \n",
    "    def __call__(self, value):\n",
    "        if not self.cmap_domain[0] <= value <= self.cmap_domain[1]:\n",
    "            raise Exception(\"Value must be inside cmap_domain.\")\n",
    "        return self.cmap(self.m(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:53:44.726555Z",
     "start_time": "2019-02-21T11:53:44.267863Z"
    }
   },
   "outputs": [],
   "source": [
    "cmap = cmap_in_range([0, 1])\n",
    "\n",
    "y_true = y.reshape(-1).numpy()\n",
    "y_pred = model(x).data.numpy().reshape(-1)\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"True\", fontsize=12)\n",
    "plt.scatter(x[:, 0], x[:, 1], color=list(map(cmap, y_true)))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Predicted\", fontsize=12)\n",
    "plt.scatter(x[:, 0], x[:, 1], color=list(map(cmap, y_pred)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cooool! Alright. Your turn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.3.1**: Can you fit a neural network like the above (maybe with more or less layers and different number of hidden neurons in each layer) to the marvel data to predict good vs. evil?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
