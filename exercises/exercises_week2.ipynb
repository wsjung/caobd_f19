{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **REMINDER: DO NOT EDIT IF INSIDE CLONED GIT FOLDER**<br>\n",
    "> \\> Go ahead and copy it and paste it in another folder. Work inside the pasted file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: A Data Scientist's most fundamental tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "Today's exercises will be related to chapters 3, 4, 5 from DSfS. The point of these exercises is to refresh your memory on some mathematics and get you comfortable doing computations in code.\n",
    "\n",
    "The exercises today cover:\n",
    "* Basic visualization\n",
    "* Linear algebra\n",
    "* Statistics\n",
    "\n",
    "**Advice**: Some of you may be new to solving problems using code. You may be wondering *what level of detail* I expect in your solutions, your code comments and explanations. **This is the guideline:** Solve the exercises in a manner that allows you to—later in life—use them as examples. This also means that you should add code comments when the code isn't self-explanatory or if you're afraid it won't make sense when you look at it with fresh eyes. You may also want to comment on your output in plain text to capture the conclusions you arrive at throughout your analysis. But express yourself succinctly. To quote (probably) Einstein: *\"Make everything as simple as possible, but not simpler\"*. Finally, when you optimize for your own future comprehension, other people will be able to understand what you did.\n",
    "\n",
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Visualization (DSFS Chapter 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.1**: The figure below meets the minumum style requirements which I expect the figures you make in this class (and life in general) should also meet:\n",
    "* Figure sizing. Try to make the aspect ratio close to 4:3.\n",
    "* Axis labels. Note that you may want to alter the `fontsize` to make them look nice.\n",
    "* Properly sized x and y tick labels.\n",
    "* Title (optional: not always necessary, but oftens helps the reader)\n",
    "* Legend (general rule: only use if you have multiple trends so reader can distinguish).\n",
    ">\n",
    "> Your task in this exercise if to reproduce this figure (perfect match not required).\n",
    ">\n",
    ">*Hint: To get figures to display inside the notebook, use the Jupyter magic `%matplotlib inline`. For pointers on how to make plots like this in Python, Google something like \"scatter plot python\" and see if you can find some examples of how other people do this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.1.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.2**: The `get_x_y` function below gives you the number of comments versus score for the latest `N` posts on a given `subreddit`.\n",
    "1. Make a scatter plot of `x` vs. `y` for the \"blackmirror\" subreddit (remember what you learned in the previous exercise about styling). Comment on what you see.\n",
    "2. Maybe you've noticed that it looks pretty bad right? That's because the data does *not scale linearly*! This is a very common thing. To visualize it you should then try to *transform* it somehow. In this case, the data scales *exponentially* in both the x and y direction. Which transformation should we use to make it linear?\n",
    "3. In two seperate figures, floating side by side, scatter plot (left) the set of x and y variables for \"blackmirror\" and (right) x and y for \"news\". Remember to transform the data. My figure looks like [this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.2b.png).\n",
    "4. Comment on any differences you see in the trends. Why might number of comments versus post upvotes look different for a TV-show than for world news?\n",
    ">\n",
    ">*Hint: By \"transformation\" I explicitly mean that you map some function onto every value in a list of values. Example: I can apply a square root transformation like `x = [np.sqrt(v) for v in x]`. A faster way to do that, of course, would be just `x = np.sqrt(x)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T21:28:54.941490Z",
     "start_time": "2019-01-23T21:28:44.166852Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "\n",
    "def get_x_y(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            x.append(d['data']['num_comments'])\n",
    "            y.append(d['data']['score'])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return x, y\n",
    "                          \n",
    "x, y = get_x_y(\"news\", 500, count=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.3**: There is clearly a huge level unevenness in the distribution of how likes and comments given to different posts. Let's visualize this using histograms!\n",
    "1. Execute `what_hist_returns = plt.hist(x)`. This should produce a histogram. But what does the variable `what_hist_returns` contain?\n",
    "2. Use `what_hist_returns` to make a similar histogram with the `plt.bar` plotting function.\n",
    "3. Plot the distributions of `np.log(x)` for \"blackmirror\" and \"news\" as histograms, side by side (you can just use the regular `plt.hist` function here). My figure looks like [this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.2c.png). Comment on the result.\n",
    ">\n",
    ">*Hint: If you didn't already know this in your heart, hardcode into it that log(0) == -inf. Because of this, when you make the log transformation of `x`, some of the posts which received zero comments will screw things up. Go ahead and try without dealing with this, and when it fails, either remove zeros from `x` or remove `-inf` values from `np.log(x)`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Linear algebra (DSFS Chapter 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.1**: What does Joel (book) mean when he uses the word *vector*? What does [Grant](https://youtu.be/fNk_zzaMoSs) mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.2**: Using `numpy`, compute:\n",
    "1. `2 * [2, 3]`,\n",
    "2. `[3, 8] + [6, 1]`,\n",
    "3. `[3, 8] * [6, 1]` and\n",
    "4. `[3, 8] · [6, 1]` (dot product)\n",
    "5. `[3, 8, 0] x [6, 1, 0]` (cross product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.3**: Imagine you have two vectors. What does it mean that the dot product between them is zero or very close to zero? What if it's very large? Intuitively, what does the dot product then measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.4**: In Data Science, we often think of matrices as (usually two-dimensional) containers for data. Let's say we collect $N=500$ data points, that each has $M=10$ features. We can loslessly represent that data using an $N \\times M$ matrix, that is a matrix that has a row for each datapoint and a column for each feature. In fact, let's just go ahead and do that by altering the code of the `get_x_y` function from before a little bit.\n",
    ">\n",
    ">*Note: `numpy` has an object type called `matrix` but we rarely use that. Instead, we represent matrices as a `numpy` object type called `array`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T07:49:55.947679Z",
     "start_time": "2019-01-18T07:49:46.008308Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_matrix(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    X = []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            X.append([\n",
    "                int(d['data']['num_comments']),\n",
    "                int(d['data']['score']),\n",
    "                int(d['data']['ups']),\n",
    "                int(d['data']['downs']),\n",
    "                len(d['data']['selftext']),\n",
    "                len(d['data']['title']),\n",
    "                int(d['data']['is_original_content']),\n",
    "                int(d['data']['spoiler']),\n",
    "                int(d['data']['num_crossposts']),\n",
    "                int(d['data']['is_video'])\n",
    "            ])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return np.array(X)\n",
    "                          \n",
    "X = get_data_matrix(\"news\", 500, count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T07:49:57.572095Z",
     "start_time": "2019-01-18T07:49:57.567762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2131,  8709,  8709, ...,     0,     3,     0],\n",
       "       [  565, 21698, 21698, ...,     0,     8,     0],\n",
       "       [  325,  3789,  3789, ...,     0,     2,     0],\n",
       "       ...,\n",
       "       [    3,    10,    10, ...,     0,     0,     0],\n",
       "       [   28,    44,    44, ...,     0,     0,     0],\n",
       "       [    7,    41,    41, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here are the data, but how can we summarize them? Let's start by finding the so-called *covariance matrix*.\n",
    "1. Use the `np.cov` method on `X` to get its $10 \\times 10$ covariance matrix.\n",
    "2. Do you notice any characteristics of this matrix that are interesting? Comment.\n",
    "3. Plot the distribution of covariances, e.g. using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.5**: There's another use of the covariance matrix, other than just learning how features co-vary. In fact, it turns out that the *eigenvectors* of the covariance matrix are a set of mutually orthogonal vectors, that point in the directions of greatest variance in the data. The eigenvector with the greatest *eigenvalue* points along the direction of greatest variation, and so on. This is pretty neat, because if we know along which axes the data is most stretched, we can figure out how best to project it when visualizing it in 2D as a scatter plot! This whole procedure has a name: **Principal Component Analysis** (PCA) and it was invented by Karl Pearson in 1901. It belongs to a powerful class of linear algebra methods called **Matrix Factorization** methods. Ok, so rather than spending too much time on the math of PCA, let's just use the `sklearn` implementation and fit a PCA on `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T08:30:52.583430Z",
     "start_time": "2019-01-18T08:30:52.577358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. Explain what the matrix you get when you call `pca.components_` means.\n",
    "2. Explain what the vector you get when you call `pca.explained_variance_ratio_` means. What insights about our data can we extract from this?\n",
    "3. Transform X using the PCA you just fitted, and scatter plot the first two dimensions of the transformed data. Please comment on what you see.\n",
    "3. Scatter plot dimensions 1 and 3 against each other. See something interesting now? What would be a way to figure out what the clusters represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Statistics (DSFS Chapter 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.1**: Take a vector `a = [1, 3, 2, 5, 3, 1, 5, 1, 9000]`:\n",
    "1. Compute the mean of `a` using `numpy`.\n",
    "2. How is median defined? Compute the median of `a` using `numpy`.\n",
    "3. For `a`, why might it make sense to take the median more seriously than the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.2**: Using the same vector `a`:\n",
    "1. How is *range* defined? Compute it.\n",
    "2. How is *variance* defined? How do variance and standard deviation relate? Compute them both. Which value is greater?\n",
    "3. What is the interquartile range? Compute it, and explain why it might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.3**: Covariance and correlation are both measures of trend similarity.\n",
    "1. How do they relate?\n",
    "2. Compute the correlation between `a` and `b = [0, 4, 1, 6, 2, 0, 6, 0, 2]`.\n",
    "3. How does that result change if you remove the last data-point from each list? Why? What *term* do we use for that last point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.4**: People often use the p-value to gauge the *significance* of a given result. If the p-value of a result is low, the result is significant (which is good) and vice versa. Intuitively, the p-value measures the probabilty that a result *could have been obtained at random*, so you can imagine that if you find that the p-value for some result is HIGH (close to one), regardless of how cool it is, people will not care because, well, you just got lucky with that measurement, didn't you? I created two lists for you below, and you are going to find out if they are *significantly* correlated. You will be using the significance threshold 0.05 (which is arbitrary, disputed, yet very standard in the literature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T09:50:21.670665Z",
     "start_time": "2019-01-24T09:50:21.666647Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can incresae this to make the data more noisy (but let it be 4 for now)\n",
    "noise_level = 4\n",
    "\n",
    "# I'm just seeding the random number generator here, so we can compare results\n",
    "np.random.seed(0)\n",
    "\n",
    "# This is your data\n",
    "x = np.arange(0, 20) + np.random.normal(size=20) * noise_level\n",
    "y = np.arange(0, 20) + np.random.normal(size=20) * noise_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T09:55:18.924607Z",
     "start_time": "2019-01-24T09:55:18.922022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Why not make a scatter plot here, to see what you're working with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. If you plotted `x` and `y` against each other, you probably noted that it looks like they correlate. Use the method `pearsonr` from `scipy.stats` to **compute the correlation coefficient**. Notice that the method also gives you the p-value of the correlation. For now, we ignore this.\n",
    ">2. So how do we figure out if this correlation strength is significant? \n",
    "The devil's advocate would surely argue, that you could obtain a similar *or stronger* correlation between completely random data.\n",
    "Well, fuck you advocate, cause we can SHOW that if you randomize the data, the correlation strength is almost always lower!\n",
    "To put this in stat-lingo, the devil's advocate believes in the so-called *Null Hypothesis*–that your result is no different from random–and the way WE reject this acvocate's pessimistic hypothesis, is simply by comparing OUR obtained correlation strength to one obtained from data we know is random, namely the *Null model*.\n",
    "The Null model, in this case just a randomized version of our existing data. \n",
    "Your job, now, is to take `x` and `y`, randomize them (each independently) and compute the correlation coefficient.\n",
    ">3. In 2. you probably found that the correlation coefficient of the Null model was lower than that of your real data.\n",
    "But that was just one example of the Null hypothesis being wrong.\n",
    "The devil's advocate needs more evidence.\n",
    "Compute again the correlation coefficient of the Null model, but do it in a `for` loop 10000 times, and report the fraction of times that this correlation coefficient is greater or equal to the correlation coefficient of your real data.\n",
    "Maybe it will happen in 0% of randomizaion trials, maybe 2%, but hopefully not more than 5%.\n",
    "Yes, you guessed, it, this fraction is indeed the p-value.\n",
    "So is it really significant?\n",
    ">4. Plot the p-value as a function of `noise_level` (let `noise_level` vary between 1 and 50), so we can see how our result becomes less and less significant as we increase the noise. Two questions: (1) at which value of `noise_level` does the correlation become insignificant, and (2) which p-value does the curve saturate at for large `noise_level`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
